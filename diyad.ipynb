{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY AD\n",
    "\n",
    "Minimalistic implementation of AD in python.\n",
    "The implementation is supposed to be minimal both in terms of linssssssssssssses-of-code and the concepts required for its implementation.\n",
    "Namely, we will use exactly one core concept: linearizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linearization can be though of as the implicit Jacobian matrix of a function (yes, the name \"linearization\" is misleading as it might imply having an offset that it does not have here).\n",
    "We need our implicit Jacobian matrix to have four attributes: a forward pass (Jacobian-Vector-Product), a backwards pass (Vector-Jacobian-Product), a reference to the parameters with respect to which we are going to differentiate to, and the value of the linearized function.\n",
    "The last attribute is required in order to amend the Jacobian.\n",
    "\n",
    "The forward pass of the Jacobian reapplies all linearized operators of our function in the order in which they were originally called.\n",
    "A simple recursion into the individual Jacobians of the operators is sufficient if each of them applies the chain rule correctly.\n",
    "The backwards pass is slightly trickier as we need to watch out for accumulating gradients.\n",
    "For this purpose of tracking accumulating gradients, our backwards pass will share a `tape` to which the gradient for each input to a function is accumulated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearization():\n",
    "    def __init__(self, p, fwd=None, bwd=None, _wrt=None):\n",
    "        self.p = (p, ) if not (isinstance(p, tuple) or p is None) else p\n",
    "        if fwd is None:\n",
    "            def fwd(*t, tape): return t[0] if len(self.p) == 1 else t\n",
    "        if bwd is None:\n",
    "            def bwd(*t, tape): return t\n",
    "        self._fwd, self._bwd = fwd, bwd\n",
    "        # Reference initial parameters w.r.t. which we want to differentiate\n",
    "        self._wrt = self if _wrt is None else _wrt\n",
    "\n",
    "    def __call__(self, *t, tape=None):\n",
    "        if tape is not None:\n",
    "            return self._fwd(*t, tape=tape)\n",
    "        # Outermost call of Linearization (i.e. without a tape yet)\n",
    "        # `tape` serves as shared dict for gradient accumulation\n",
    "        tape = [0.] * len(t)\n",
    "        o = self._fwd(*t, tape=tape)\n",
    "        return o if self.p is not None else tuple(tape)\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.__class__(None, self._bwd, self._fwd, _wrt=self._wrt)\n",
    "\n",
    "    @classmethod\n",
    "    def chain(clss, lins, p, fwd, bwd):\n",
    "        lins = (lins, ) if isinstance(lins, Linearization) else lins\n",
    "\n",
    "        def chained_fwd(*t, tape=None):\n",
    "            # NOTE, `t` is the full input of our overall model. All\n",
    "            # linearizations that come after must by definition work on all of\n",
    "            # `t`. Thus, we can skip bookkeeping which input gets where.\n",
    "            return fwd(*(l(*t, tape=tape) for l in lins), tape=tape)\n",
    "\n",
    "        def chained_bwd(*t, tape=None):\n",
    "            bs = bwd(*t, tape=tape)\n",
    "            bs = (bs, ) if len(lins) == 1 else bs\n",
    "            assert isinstance(bs, tuple) and len(bs) == len(lins)\n",
    "            for l, b in zip(lins, bs):\n",
    "                o = l.T(*b, tape=tape)\n",
    "                assert isinstance(o, tuple) or o is None\n",
    "                if l is lins[0]._wrt:\n",
    "                    assert len(o) == len(tape)\n",
    "                    for i, ne in enumerate(o):\n",
    "                        tape[i] += ne\n",
    "            return None  # all information is stored on tape\n",
    "\n",
    "        assert all(l._wrt is lins[0]._wrt for l in lins)\n",
    "        return clss(p, chained_fwd, chained_bwd, _wrt=lins[0]._wrt)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.p}, {self._fwd}, {self._bwd})\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining two simple functions (and their linearizations): `exp` and `add`.\n",
    "First, let's have a look at the unary function `exp`.\n",
    "Its Jacobian is a diagonal matrix as it is a point-wise operator.\n",
    "In terms of code, the forward and reverse of the linearization are thus simple multiplications.\n",
    "The diagonal operator, i.e. the multiplication, is amended once to the left of the Jacobian for the forward pass and once to the right for the backwards pass.\n",
    "\n",
    "Notice how in the implementation below the computation of the diagonal is hoisted out of the forward and reverse mode using a closure.\n",
    "This specific choice of rematerialization strategy can be easily configured by writing custom rules for functions.\n",
    "A custom rule is nothing more than a simple `isinstance` check for a `Linearization` in a function.\n",
    "\n",
    "The binary operator `add` is slightly more elaborate in that it takes two arguments and has to handle all permutations of linearizations versus no linearizations as input.\n",
    "The `isinstance` checks thus have four cases but conceptually works the same as `exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(pl):\n",
    "    if isinstance(pl, Linearization):\n",
    "        y = exp(*pl.p)\n",
    "        def fwd(t, tape):\n",
    "            return y * t\n",
    "        def bwd(*t, tape):\n",
    "            assert len(t) == 1\n",
    "            return (y * t[0], )\n",
    "        return Linearization.chain(pl, y, fwd, bwd)\n",
    "    return np.exp(pl)\n",
    "\n",
    "\n",
    "def add(pl_l, pl_r):\n",
    "    if isinstance(pl_l, Linearization) and isinstance(pl_r, Linearization):\n",
    "        def fwd(t0, t1, tape):\n",
    "            return t0 + t1\n",
    "        def bwd(*t, tape):\n",
    "            assert len(t) == 1\n",
    "            return (t, t)\n",
    "        assert len(pl_l.p) == len(pl_r.p) == 1\n",
    "        return Linearization.chain((pl_l, pl_r), pl_l.p[0] + pl_r.p[0], fwd, bwd)\n",
    "    elif any(isinstance(pl, Linearization) for pl in (pl_l, pl_r)):\n",
    "        pll, p = (pl_l, pl_r) if isinstance(pl_l, Linearization) else (pl_r, pl_l)\n",
    "\n",
    "        def fwd(*t, tape):\n",
    "            assert len(t) == 1\n",
    "            return t[0]\n",
    "        def bwd(*t, tape):\n",
    "            assert len(t) == 1\n",
    "            return t\n",
    "        assert len(pll.p) == 1\n",
    "        return Linearization.chain(pll, pll.p[0] + p, fwd, bwd)\n",
    "    else:\n",
    "        return pl_l + pl_r\n",
    "\n",
    "\n",
    "def cost(p):\n",
    "    p = exp(p)\n",
    "    y = exp(add(p, -25))\n",
    "    return add(y, p)\n",
    "\n",
    "\n",
    "p0, t = 3.14, 2.\n",
    "p0lin = Linearization(p0)\n",
    "j = cost(p0lin)\n",
    "j(t), j.T(t), jax.jvp(lambda p: jax.numpy.exp(jax.numpy.exp(p) - 25) + jax.numpy.exp(p), (p0, ), (t, ))[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = 1e-2 * np.arange(0, 9)\n",
    "ones = np.ones((9, ))\n",
    "y = exp(p0)\n",
    "j = exp(exp(Linearization(p0)))\n",
    "j(p0), j.T(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, jj_T = jax.vjp(lambda x: jax.numpy.exp(jax.numpy.exp(x)), p0.astype(float))\n",
    "_, jj_at_p0 = jax.jvp(lambda x: jax.numpy.exp(jax.numpy.exp(x)), (p0.astype(float), ), (p0, ))\n",
    "jj_at_p0, jj_T(ones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How best to rematerialize in the Jacobian is a difficult question.\n",
    "The approach taken here is a flexible albeit lazy one: We let the user do it explicitly when defining the Jacobian.\n",
    "However, the design of building forward and backwards pass in general incentivizes sharing memory between both passes.\n",
    "Furthermore, it is trivial to hoist out constants via closures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_reduction(pl, n=32, n_cols=3):\n",
    "    if isinstance(pl, Linearization):\n",
    "        def fwd(*t, tape):\n",
    "            return weighted_reduction(*t)\n",
    "        def bwd(*t, tape):\n",
    "            assert len(t) == 1 and len(pl.p) == 1\n",
    "            p_shp = pl.p[0].reshape(3, -1).shape\n",
    "            t_T, indices = np.zeros(p_shp), np.arange(n) % p_shp[0]\n",
    "            for i, idx in enumerate(indices):\n",
    "                super_expensive_weights = np.ones(p_shp[1:])\n",
    "                t_T[idx] += t[0][i] * super_expensive_weights\n",
    "            return (t_T.reshape(pl.p[0].shape), )\n",
    "        return Linearization.chain(pl, weighted_reduction(*pl.p), fwd, bwd)\n",
    "    p = pl.reshape(n_cols, -1)\n",
    "    y, indices = np.zeros((n, )), np.arange(n) % p.shape[0]\n",
    "    for i, idx in enumerate(indices):\n",
    "        super_expensive_weights = np.ones(p.shape[1:])\n",
    "        y[i] = np.sum(p[idx] * super_expensive_weights)\n",
    "    return y\n",
    "\n",
    "\n",
    "p0lin = Linearization(np.arange(12, dtype=float))\n",
    "y2 = weighted_reduction(*p0lin.p)\n",
    "j = weighted_reduction(p0lin)\n",
    "print(y2)\n",
    "j(*p0lin.p), j.T(y2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(p): return weighted_reduction(exp(p))\n",
    "\n",
    "p0 = 1e-2 * np.arange(0, 9)\n",
    "f0 = f(p0)\n",
    "f_ones = np.ones(f0.shape)\n",
    "\n",
    "j = f(Linearization(p0))\n",
    "j(p0), j.T(f_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(pl):\n",
    "    if isinstance(pl, Linearization):\n",
    "        def fwd(*t, tape):\n",
    "            return sum(*t)\n",
    "        def bwd(*t, tape):\n",
    "            assert len(t) == 1 and len(pl.p) == 1\n",
    "            return (t[0] * np.ones(pl.p[0].shape), )\n",
    "        return Linearization.chain(pl, sum(*pl.p), fwd, bwd)\n",
    "    return np.sum(pl)\n",
    "\n",
    "p0 = Linearization(np.arange(0, 9, dtype=float))\n",
    "j = sum(p0)\n",
    "j(*p0.p), j.T(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow(pl, exponent):\n",
    "    if isinstance(pl, Linearization) and not isinstance(exponent, Linearization):\n",
    "        yl = exponent * pow(*pl.p, exponent - 1)\n",
    "        def fwd(*t, tape):\n",
    "            assert len(t) == 1\n",
    "            return yl * t[0]\n",
    "        def bwd(*t, tape):\n",
    "            assert len(t) == 1\n",
    "            return (yl * t[0], )\n",
    "        return Linearization.chain(pl, pow(*pl.p, exponent), fwd, bwd)\n",
    "    elif not isinstance(pl, Linearization) and not isinstance(exponent, Linearization):\n",
    "        return pl**exponent\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "y = pow(3., 3)\n",
    "j = pow(Linearization(3.), 3)\n",
    "j.T(4.), jax.vjp(lambda x: x**3, 3.)[1](4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 0.\n",
    "\n",
    "def h(p):\n",
    "    return add(sum(pow(add(-data, weighted_reduction(exp(p))), 2)), sum(pow(p, 2)))\n",
    "\n",
    "p0 = Linearization(np.arange(0, 9, dtype=float))\n",
    "j = h(p0)\n",
    "j(*p0.p), j.T(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
